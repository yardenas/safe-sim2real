from typing import Callable, Protocol

import jax
import jax.numpy as jnp
from brax.training.types import Params, Transition
from jax.scipy.stats import norm


class QTransformation(Protocol):
    def __call__(
        self,
        transitions: Transition,
        q_fn: Callable[[Params, jax.Array], jax.Array],
        policy: Callable[[jax.Array], tuple[jax.Array, jax.Array]],
        gamma: float,
        alpha: jax.Array | None = None,
        scale: float = 1.0,
        key: jax.Array | None = None,
    ):
        ...


class UCBCost(QTransformation):
    def __call__(
        self,
        transitions: Transition,
        q_fn: Callable[[Params, jax.Array], jax.Array],
        policy: Callable[[jax.Array], tuple[jax.Array, jax.Array]],
        gamma: float,
        alpha: jax.Array | None = None,
        scale: float = 1.0,
        key: jax.Array | None = None,
    ):
        next_action, _ = policy(transitions.next_observation)
        next_q = q_fn(transitions.next_observation, next_action)
        next_v = next_q.mean(axis=-1)
        disagreement = transitions.extras["state_extras"]["disagreement"]
        cost = transitions.extras["state_extras"]["cost"] + disagreement
        target_q = jax.lax.stop_gradient(
            cost * scale + transitions.discount * gamma * next_v
        )
        return target_q


class PessimisticCostUpdate(QTransformation):
    def __call__(
        self,
        transitions: Transition,
        q_fn: Callable[[Params, jax.Array], jax.Array],
        policy: Callable[[jax.Array], tuple[jax.Array, jax.Array]],
        gamma: float,
        alpha: jax.Array | None = None,
        scale: float = 1.0,
        key: jax.Array | None = None,
    ):
        next_action, _ = policy(transitions.next_observation)
        next_q = q_fn(transitions.next_observation, next_action)
        next_v = next_q.mean(axis=-1)
        cost = transitions.extras["state_extras"]["cost"]
        target_q = jax.lax.stop_gradient(
            cost * scale + transitions.discount * gamma * next_v
        )
        target_q = q_fn(transitions.observation, transitions.action).mean(axis=-1)
        # # TODO: check if works (intersection of models)
        # target_q = jax.lax.stop_gradient(jnp.maximum(new_target_q, old_q)) #FIXME: Change back to min?
        return target_q


class RAMU(QTransformation):
    """
    https://arxiv.org/pdf/2301.12593
    """

    def __init__(self, epsilon: float, n_samples: int, wang_eta: float) -> None:
        self.epsilon = epsilon
        self.n_samples = n_samples
        self.wang_eta = wang_eta

    def __call__(
        self,
        transitions: Transition,
        q_fn: Callable[[Params, jax.Array], jax.Array],
        policy: Callable[[jax.Array], tuple[jax.Array, jax.Array]],
        gamma: float,
        alpha: jax.Array | None = None,
        scale: float = 1.0,
        key: jax.Array | None = None,
    ):
        if isinstance(transitions.observation, dict):
            sampled_next_obs = {
                k: ramu_sample(
                    self.epsilon,
                    self.n_samples,
                    transitions.observation[k],
                    transitions.next_observation[k],
                    key,
                )
                for k in transitions.observation
            }
        else:
            sampled_next_obs = ramu_sample(
                self.epsilon,
                self.n_samples,
                transitions.observation,
                transitions.next_observation,
                key,
            )
        next_action, _ = policy(sampled_next_obs)
        next_q = q_fn(sampled_next_obs, next_action)
        next_v = next_q.mean(axis=-1)
        next_v = wang(self.n_samples, self.wang_eta, next_v)
        cost = transitions.extras["state_extras"]["cost"]
        target_q = jax.lax.stop_gradient(
            cost * scale + transitions.discount * gamma * next_v
        )
        return target_q


def ramu_sample(epsilon, n_samples, observation, next_observation, key):
    delta = next_observation - observation
    x = jax.random.uniform(
        key,
        (n_samples, *delta.shape),
        minval=-2.0 * epsilon,
        maxval=2.0 * epsilon,
    )
    return observation + delta * (1.0 + x)


def wang(n_samples, wang_eta, next_v, descending=True):
    quantiles = jnp.linspace(0, 1, n_samples + 1)
    quantiles = norm.cdf(norm.ppf(quantiles) + wang_eta)
    probs = (quantiles[1:] - quantiles[:-1]) * n_samples
    next_v = (jnp.sort(next_v, axis=0, descending=descending) * probs[:, None]).mean(0)
    return next_v


class SACBase(QTransformation):
    def __init__(self, use_bro: bool = True) -> None:
        self.use_bro = use_bro

    def __call__(
        self,
        transitions: Transition,
        q_fn: Callable[[Params, jax.Array], jax.Array],
        policy: Callable[[jax.Array], tuple[jax.Array, jax.Array]],
        gamma: float,
        alpha: jax.Array | None = None,
        scale: float = 1.0,
        key: jax.Array | None = None,
    ):
        next_action, next_log_prob = policy(transitions.next_observation)
        next_q = q_fn(transitions.next_observation, next_action)
        if self.use_bro:
            next_v = next_q.mean(axis=-1)
        else:
            next_v = next_q.min(axis=-1)
        next_v -= alpha * next_log_prob
        target_q = jax.lax.stop_gradient(
            transitions.reward * scale + transitions.discount * gamma * next_v
        )
        return target_q


class RAMUReward(QTransformation):
    def __init__(
        self, epsilon: float, n_samples: int, wang_eta: float, use_bro: bool
    ) -> None:
        self.epsilon = epsilon
        self.n_samples = n_samples
        self.wang_eta = wang_eta
        self.use_bro = use_bro

    def __call__(
        self,
        transitions: Transition,
        q_fn: Callable[[Params, jax.Array], jax.Array],
        policy: Callable[[jax.Array], tuple[jax.Array, jax.Array]],
        gamma: float,
        alpha: jax.Array | None = None,
        scale: float = 1.0,
        key: jax.Array | None = None,
    ):
        if isinstance(transitions.observation, dict):
            sampled_next_obs = {
                k: ramu_sample(
                    self.epsilon,
                    self.n_samples,
                    transitions.observation[k],
                    transitions.next_observation[k],
                    key,
                )
                for k in transitions.observation
            }
        else:
            sampled_next_obs = ramu_sample(
                self.epsilon,
                self.n_samples,
                transitions.observation,
                transitions.next_observation,
                key,
            )
        next_action, next_log_prob = policy(sampled_next_obs)
        next_q = q_fn(sampled_next_obs, next_action)
        if self.use_bro:
            next_v = next_q.mean(axis=-1)
        else:
            next_v = next_q.min(axis=-1)
        next_v = wang(self.n_samples, self.wang_eta, next_v)
        next_v -= alpha * next_log_prob
        target_q = jax.lax.stop_gradient(
            transitions.reward * scale + transitions.discount * gamma * next_v
        )
        return target_q


class LCBReward(QTransformation):
    def __init__(self, use_bro: bool = True):
        self.use_bro = use_bro

    def __call__(
        self,
        transitions: Transition,
        q_fn: Callable[[Params, jax.Array], jax.Array],
        policy: Callable[[jax.Array], tuple[jax.Array, jax.Array]],
        gamma: float,
        alpha: jax.Array | None = None,
        scale: float = 1.0,
        key: jax.Array | None = None,
    ):
        next_action, next_log_prob = policy(transitions.next_observation)
        next_q = q_fn(transitions.next_observation, next_action)
        if self.use_bro:
            next_v = next_q.mean(axis=-1)
        else:
            next_v = next_q.min(axis=-1)
        next_v -= alpha * next_log_prob
        disagreement = transitions.extras["state_extras"]["disagreement"]
        reward = transitions.reward - disagreement
        target_q = jax.lax.stop_gradient(
            reward * scale + transitions.discount * gamma * next_v
        )
        return target_q


class SACCost(QTransformation):
    def __call__(
        self,
        transitions: Transition,
        q_fn: Callable[[Params, jax.Array], jax.Array],
        policy: Callable[[jax.Array], tuple[jax.Array, jax.Array]],
        gamma: float,
        alpha: jax.Array | None = None,
        scale: float = 1.0,
        key: jax.Array | None = None,
    ):
        next_action, _ = policy(transitions.next_observation)
        next_q = q_fn(transitions.next_observation, next_action)
        next_v = next_q.mean(axis=-1)
        cost = transitions.extras["state_extras"]["cost"]
        target_q = jax.lax.stop_gradient(
            cost * scale + transitions.discount * gamma * next_v
        )
        return target_q


def get_cost_q_transform(cfg):
    if (
        "cost_robustness" not in cfg.agent
        or cfg.agent.cost_robustness is None
        or cfg.agent.cost_robustness.name == "neutral"
    ):
        return SACCost()
    if cfg.agent.cost_robustness.name == "ramu":
        del cfg.agent.cost_robustness.name
        robustness = RAMU(**cfg.agent.cost_robustness)
    elif cfg.agent.cost_robustness.name == "ucb_cost":
        robustness = UCBCost()
    elif cfg.agent.cost_robustness.name == "pessimistic_cost_update":
        robustness = PessimisticCostUpdate()
    else:
        raise ValueError("Unknown robustness")
    return robustness


def get_reward_q_transform(cfg):
    if (
        "reward_robustness" not in cfg.agent
        or cfg.agent.reward_robustness is None
        or cfg.agent.reward_robustness.name == "neutral"
    ):
        return SACBase(use_bro=cfg.agent.use_bro)
    if cfg.agent.reward_robustness.name == "ramu":
        del cfg.agent.reward_robustness.name
        robustness = RAMUReward(
            **cfg.agent.reward_robustness, use_bro=cfg.agent.use_bro
        )
    elif cfg.agent.reward_robustness.name == "lcb_reward":
        robustness = LCBReward(use_bro=cfg.agent.use_bro)
    else:
        raise ValueError("Unknown robustness")
    return robustness
