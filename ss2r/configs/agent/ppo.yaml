defaults:
  - cost_robustness: null
  - reward_robustness: null
  - penalizer: null
  - propagation: null

name: ppo
max_devices_per_host: null
learning_rate: 0.0001
entropy_cost: 0.0001
discounting: 0.9
safety_discounting: 0.9
unroll_length: 10
batch_size: 32
num_minibatches: 16
num_updates_per_batch: 2
normalize_observations: false
reward_scaling: 1.0
cost_scaling: 1.0
clipping_epsilon: 0.3
gae_lambda: 0.95
safety_gae_lambda: 0.0
deterministic_eval: true
normalize_advantage: true
policy_hidden_layer_sizes: [128, 128, 128]
value_hidden_layer_sizes: [128, 128, 128]
activation: tanh
num_trajectories_per_env: 1
